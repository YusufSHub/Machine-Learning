{"cells":[{"metadata":{},"cell_type":"markdown","source":"   **Veri Bilimini öğrenme aşamamda birçok kaynaktan edindiğim bilgileri bir araya getirerek bir not defteri oluşturdum. Bu not defteri\nüzerinde veriyi, veriyi işlemeyi, ve veriden anlamlı bilgiler çıkarmak için bilinmesi gerekenleri özet halinde öğreneceksiniz.**"},{"metadata":{},"cell_type":"markdown","source":"### Veri Madenciliği Süreci\n\n![crisp](https://i.ibb.co/F0GyDzK/crisp-dm.png)"},{"metadata":{},"cell_type":"markdown","source":"### Genel Süreç Adımları\n\n\n1) Problemin(İşin) Anlaşılması:\n\n Verileri doğru değerlendirerek, projede başarılı olmanın ilk adımı, uygulamanın hangi amaçla yapılacağını doğru bir şekilde anlamaktır.\n \n2) Verinin Anlaşılması\n\n Öznitelik değerlerinin dağılımı, outlier değerlerin anlaşılması, hangi öznitelik değerlerinin projede özellikle önem taşıyacağı konusunda fikir edinmek gerekir.\n\n3) Verinin Hazırlanması\n\nTanımlanan problem için gerekli olan veriler hazırlanmalıdır. Daha sonra veriler ön işleme(preprocessing) adımlarında temizleme-döndürme-indirgeme gibi işlemlerle düzeltilmelidir.\n\n4) Modelin Kurulması\n\nSupervised(gözetimli) ya da unsupervised(gözetimsiz) algoritmalara bakılarak uygun yöntemlerin belirlenmesi gerekir. Yöntemlerin bir arada kullanılması olarak tanımlanan ensemble yaklaşımları kullanılabilir.\n\n5) Yorumlama\n\nÇıkan sonuçlar üzerinde görselleştirme yapılarak sonuçların doğru bir şekilde anlaşılması gerekir.\n"},{"metadata":{},"cell_type":"markdown","source":"# Python Kütüphaneleri\n\n## 1. NUMPY\n\nNumpy(Numerical Pyhton), bilimsel hesaplama işlemlerini kolaylaştırmak için yazılmış bir python kütüphanesidir. Temelini numpy dizileri oluşturur. Pyhton dizilerine benzer fakat hız ve işlevsellik açısından daha iyidir. Arrayler ve matrisler üzerinde çalışır.\n\n\n"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# TEMEL İŞLEMLER\n\nimport numpy as np\n\n# array oluşturmak\n\nx = np.array([12,3,4,50, 40])\ny = np.array([15,5,6,51, 4])\n\n# iki arrayi birleştirme\n\nnp.concatenate([x,y])\n\n# array ayırma\n\na,b,c=np.split(x,[3,5])\n\n# array sıralama\n\nnp.sort(x)\n\n# eleman işlemleri\n\na[0] = 1 # 0. indise 1 değerini atar\na[:,0] #0. Sütuna erişir\na[0,:] #0. Satıra erişir\n\n# koşullu eleman işlemleri\n\nnp.sum(a > 10) #array içinde 10 dan büyük olan tüm sayıları say\nnp.all(a > 5) # tüm elemanlar için bool tipinde sonuç döner\n\n# matematiksel işlemler\n\nnp.add(a,3) # a'nın her elamnına 3 ekler\nnp.substract() # çıkarma\nnp.multiply() # çarpma\nnp.divide() # bölme\n\nnp.mean(a) # ortalaması\nnp.add.reduce(a) #elemanlarını topla\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. PANDAS\n\nVeri manipülasyonu ve veri analizi için yazılmış açık kaynaklı bir Python kütüphanesidir. Veri işleme ve temizleme işlerini yapar. Makine öğreniminde yaygın olarak kullanılır. Ekonomik ve finansal çalışmalar için kullanılır. Numphy kütüphanesinin özelliklerini kullanır ve genişletir. Pandas kendi içinde seriler ve veri çerçeveleri(dataframe) olarak ikiye ayrılır.\n"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Temel İşlemler\n\nimport pandas as pd\n\n# pandas serisi oluşturma\n\nseri=pd.Series([1,2,3,4,5])\n\n# numpy arrayi üzerinden seri oluşturma\n\na=np.array([1,2,3,4,5])\nseri=pd.Series(a)\n\n# iki seriyi birleştirme\n\npd.concat([seri1,seri2])\n\n# serilerde eleman işlemleri\n\nseri=pd.Series([121,200,150,99],index=[\"reg\",\"loj\",\"cart\",\"rf\"])\n\nseri.index\nseri.keys\nlist(seri.items)\nseri.values\n\n# dataframe oluşturma\n\np=[1,2,3,4,5]\npd.DataFrame(p.columns=['degisken_ismi'])\n\n# dataframe eleman işlemleri\n\ndf.drop(‘a’,axis=0,inplace=True) # Kalıcı olarak silme işlemi\ndf[‘var’] is df[‘var1’] # aynı mı diye sorar ve bool döner\n\n# listeleme\n\ndf.loc[0:3]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. SEABORN\n\nKlasik veri formatı üzerinde veri görselleştirmesi yaparak daha anlaşılır hale getirmek gerekir. Pyhton’da veri görselleştirmesi yaparken genelde Matplotlib kütüphanesi kullanılmaktadır. Seaborn ise alternatif olarak kullanılan daha az kod yazarak daha güzel grafikler ortaya çıkarmanızı sağlayan bir kütüphanedir. Matplotlib kütüphanesine yüksek seviye ara yüz sağlayan bir kütüphanedir. Seaborn ile;\n\n* Estetik olarak hoş olan varsayılan temaları kullanma\n* Özel olarak renk paleti belirleme\n* Göz alıcı istatistiksel grafikler yapma\n* Dağılımları kolayca ve esnek bir şekilde sergileme\n* Matris ve DataFrame içindeki bilgileri görselleştirme yapılabilir.\n\nSeaborn, keşifçi analiz için kullanılmaktadır. Veri hakkında hızlı bir şekilde bilgi sahibi olmamızı sağlar.\n"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Veri Seti Üzerinde Temel İşlemler\nimport seaborn as sns\n\n# Veri Seti Yapısal Bilgileri\n\ndf.info() # veriseti bilgilerini içerir.\ndf.dtypes() # sadece değişkenler ve değişken bilgilerini verir.\n\n# Veri Seti Betimlenmesi\n\ndf.shape() # gözlem ve değişken sayısını verir.\ndf.columns # sadece değişken isimleri\ndf.describe() # betimsel istatistikleri verir.\n\n# Eksik Değerlerin İncelenmesi\n\ndf.isnull().any() # eksik değer varsa false döner\ndf.isnull().sum() # hangi değişkenlerde kaçar tane eksik var.\ndf[\"mass\"].fillna(df.mass.mean().inplace=True) # mass değişkeninin boş değelerine ortalmasını yazar.\n\n# Kategorik Değişken Özetleri\n\nkat_df=df.select.dtypes(include=[“object”]) # kategorik değişkenleri sıralar\nkat_df=method.unique() # kategorik sınıfları yazar.\nKat_df[“method”].value_counts().count() # methos isimli kategorik değişkenin kaç sınıfı var\n\n# Sürekli Değişken Özetleri\ndf_num=df.select_dtypes(include=[“float64”,”int64”]) # sayısal değişkenleri yazar.\ndf_num[“distance”].describe().T # distance değişkeni için mean,std,avg gibi değerleri yazar.\n\n\n# Temel Grafik İşlemleri\n\n# Kütüphaneleri ve Verileri Ekleme:\n \ndiamonds = sns.load_dataset('diamonds')\ndf = diamonds.copy()\n\n# Sütun Grafiğinin Oluşturulması(BarPlot):\n\ndf[\"cut\"].value_counts().plot.barh().set_title(\"Cut Değişkeninin Sınıf Frekansları\"); # pandas ile \nsns.barplot(x = \"cut\", y = df.cut.index, data= df) # seaborn ile\n\n# Çaprazlamalar:\nsns.catplot(x = \"cut\", y = \"price\", data = df) # Renk üzerinde noktalarla yoğunlaşma gösterilir.\n\n# Histogram ve Yoğunluk Grafiğinin Oluşturulması:\nsns.distplot(df.price, kde = False) # (bins=1000 eklersek histogram hassaslaşır.)\n\n# Histogram ve Yoğunluk Çaprazlamaları:\n(sns\n .FacetGrid(df,\n              hue = \"cut\",\n              height = 5,\n              xlim = (0, 10000))\n .map(sns.kdeplot, \"price\", shade= True)\n .add_legend()\n)\n\n# Kutu Grafik(BoxPlot) Oluşturma:\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df) \nsns.boxplot(x = \"day\", y = \"total_bill\", hue = \"sex\", data = df)\n\n# Violin Grafiği:\n\nsns.catplot(y = \"total_bill\", kind = \"violin\", data = df)\nsns.catplot(x= \"day\", y = \"total_bill\", kind = \"violin\", data = df)\n\n# Korelasyon Grafikleri:\n\n# Scatterplot:Sayısal değişkenler arasındaki ilişkiyi gösterir.\nsns.scatterplot(x = \"total_bill\", y = \"tip\", data = df)\nsns.scatterplot(x = \"total_bill\", y = \"tip\", hue = \"time\",data = df)\nsns.scatterplot(x = \"total_bill\", y = \"tip\", hue = \"time\", style = \"time\", data = df)\n\n# Doğrusal İlişkinin Gösterilmesi:\n\nsns.lmplot(x = \"total_bill\", y = \"tip\", data = df) # scatterplot grafiğinin orasında bir doğru olur.\nsns.lmplot(x = \"total_bill\", y = \"tip\", hue = \"smoker\", data = df)\n\n# ScatterPlot Matrisi:\n\nSayısal değişkenler için bir saçılım grafiğidir.\nsns.pairplot(df);=>dataset içindeki tüm değişkenlerin birbirleri ile olan ilişkilerini gösteren grafikler verir\nsns.pairplot(df, hue = \"species\");=>species değişkenine göre renklendirebiliriz\n\n# Isı Haritası(Heat Map):\n \n # Yıl ve ay gibi değişkenler varsa, çok sınıflı bir dataset ise daha iyi sonuç verir. Dataframe pivot tablo olmalı yoksa çalışmaz.\nsns.heatmap(df);\nsns.heatmap(df, annot = True, fmt = \"d\")\n# Çizgi Grafik(line plot):\n\n# Nesnelerin interneti gibi daha zor problemlerde kullanılır.\n\nsns.lineplot(x = \"timepoint\", y = \"signal\", data = df)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ÖLÇÜ SKALASI\n\n**1. Kategorik Değişken**\n*  Nominal(İsimsel) : Değerler küçüklük büyüklük bakımından sıralanamıyorsa, herhangi bir sınıflandırma yapılmıyorsa nominaldir.\n*  Ordinal(Sıralı) : Değerler büyüklük küçüklük skalasına göre sıralanabilir.\n\n**2. Sürekli Değişken**\n*  Interval  (Aralıklı) : Kullanılan sayılar arasındaki uzaklıklar ölçülebilmektedir. Değerler için bir 0 noktası bulunmamaktadır.\n*  Ratio (Oransal) : Doğal bir 0 noktası bulunmaktadır. Uzunluk ve ağırlık ölçüleri en temel örneğidir.\n"},{"metadata":{},"cell_type":"markdown","source":"### Veri Kalitesi\n\nVerileri incelerken ortaya çıkan hataları iki farklı kategoride inceleyebiliriz. Ölçüm cihazının hatalı kalibre edilmesi nedeni ile ortaya çıkan hatalar **sistematik** hatadır. Ölçüm sırasında tesadüfi olarak ölçümün bozulmasına neden olan etkenler sonucunda ortaya çıkan hatalar ise **tesadüfi** hatadır.\n"},{"metadata":{},"cell_type":"markdown","source":"# Veri Ön İşleme (Data Preprocessing)\n\nVeri analizinin sonuçlarının optimum seviyede olmasını sağlamak amacıyla bu adım dikkatli bir şekilde gerçekleştirilmelidir.\n\n\n1) Veri Entegrasyonu\n\n Verinin bir çok kaynaktan toplanması, seçilmesi ve entegre edilerek tek bir kaynakta bir araya getirilmesi işlemidir.\n\n2) [Veri Temizleme (Data Cleaning)](#1)\n\nVerideki parazitlerin ortadan kaldırılması, eksik ve aykırı veri(outlier-missing values) sorunlarının çözülmesi gerekir.\n\n3) [Veri Dönüştürme (Data Transformation)](#2)\n\nNormalization ve Satandardization gibi işlemlerin uygulanarak verinin standart bir dağılıma sahip olmasını sağlamak gerekir.\n\n4) [Veri İndirgeme](#3)\n\nÖznitelik(feature) sayısını, örnekleme(sampling) faktör analizi(faktör analysis), boyut indirgeme(dimension reduction) gibi tekniklerle azaltılması, veri hacminin küçültülmesi işlemleridir.\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# 2. Veri Temizleme\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Eksik Veri Gözlemi(Mising Values)\n\nEksik veri(missing value/data) bir veya daha fazla gözlem/öznitelik değerinin, sistematik olmayan bir veri giriş hatası veya cevap vericinin özellikle cevap vermemesi gibi nedenlerle veri dizisinde eksik kalmasıdır. \n\n*Eksik veriyi saptama:*\n\n> df.isnull().sum()\n\n*Eksik verileri silme*\n\n> df.dropna(axis = 1, how = \"all\", inplace = True)\n\n*Eksik verilere değer atama*\n\n> df[\"V1\"].fillna(df[\"V1\"].mean())\n\n> df[\"V3\"].fillna(df[\"V3\"].median())\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 2. Aykırı Veri Gözlemi (Outliers)\n\n  Outlier, en küçük ve en büyük değerleri de içermekle birlikte, bir veri dizisinde normal beklentilerin dışında yer alan, sahip oldukları tekil özellikleri ile diğer nesnelerden açıkça farklılık gösteren sıra dışı nesnelerdir. Aykırı gözlem analizi, günümüzde kredi kartı dolandırıcılığından bilgisayar ağı ihlallerine, pazarlamada aşırı düşük veya yüksek gelirleri olan müşterilerin satın alma davranışlarının keşfedilmesine kadar çeşitli alanlarda başarılı sonuçlar alınmasını sağlamaktadır.\n  \n  \n  ![outlier](https://i.ibb.co/X538DYT/outlier-scatterplot.png)\n  \n\n#### *Outlier saptama:*\n\n* Sektör bilgisi ile anlaşılabilir.\n* Bir değişkenin ortalamasının üzerine aynı değişkenin standart sapması hesaplanarak eklenir. 1,2 ya da 3 standart sapma değeri ortalama üzerine eklenerek ortaya çıkan bu değer eşik değeri olarak düşünülür. Bu değerden yukarıda ya da aşağıda olan değerler aykırı gözlem olarak tanımlanır.\n* Z-Skoru Yaklaşımı : Değişken standartlaştırılır. Örnek olarak dağılımın sağından ve solundan -+2,5 değerine bir eşik değeri konulur. Bu değerin üzerinde ya da altında olanlar aykırı değer olur.\n* Boxplot Yöntemi : Değişkenin değerleri küçükten büyüğe doğru sıralanır. Çeyrekliklerin(yüzdeliklerine) yani Q1,Q3 değerlerine karşılık değerler üzerinden bir eşik değer hesaplanır. Bu değere göre aykıdı değer tanımı yapılır.\n\n\n> Q1 = df_table.quantile(0.25)\n\n> Q3 = df_table.quantile(0.75)\n\n> IQR = Q3-Q1\n\n> alt_sinir = Q1- 1.5*IQR\n\n> alt_sinir = Q1- 1.5*IQR\n\n![outliers](https://i.ibb.co/LkJvZRM/abv.png)\n\n\n\n#### *Outlier sorununu çözmek:*\n\n* Silme Yaklaşımı:\n\n> t_df = df_table[~((df_table < (alt_sinir)) | (df_table > (ust_sinir))).any(axis = 1)]\n\n* Ortalama ile Doldurma:\n\n> df_table[aykiri_tf] = df_table.mean()\n\n* Baskılama Yöntemi:\n\n> df_table[aykiri_tf] = alt_sinir\n\n\n#### *Çok Değişkenli Aykırı Gözlem Analizi -Local Outlier Factor(LOF)-* \n*Yoğuluk Temelli Yaklaşım:*\n\n İki değişken tek başınayken aykırı değer olarak görülmüyorken, iki gözlem eş zamanlı olarak değerlendirildiğinde aykırı değerler olabilir.Gözlemleri bulundukları konumda yoğunluk tabanlı skorlayarak buna göre aykırı değer olabilecek değerleri tanımlayabilmemize imkan sağlıyor. Bir noktanın local yoğunluğu bu noktanın komşuları ile karşılaştırılıyor. Eğer bir nokta komşularınının yoğunluğundan anlamlı şekilde düşük ise bu nokta komşularından daha seyrek bir bölgede bulunuyordur yorumu yapılabiliyor. Dolayısıyla burada bir komşuluk yapısı söz konusu. Bir değerin çevresi yoğun değilse demek ki bu değer aykırı değerdir şeklinde değerlendiriliyor. \n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 3. Veri Dönüştürme\n\n"},{"metadata":{},"cell_type":"markdown","source":"Data transformation, daha sağlıklı sonuçların elde edilmesi veya verinin kullanılan algoritmalarla uyumlu olabilmesi için verinin tanımalanan bir fonksiyona uygun olarak farklı değer veya ölçeklere dönüştürülmesi işlemidir.\n\n**A) Veri Normalleştirme**\n\nVeri dizisinde bulunan değerlerin [-1,+1] veya [0,+1] aralığında yer alacak şekilde dönüştürülmesidir. Bu yöntem farklı büyüklükte verinin bulunduğu dizilerde vazgeçilmez dönüştürme işlemlerinin başında gelmektedir.\n\n*Normalleştirme:*\n\nDeğişkenlerin 0 ve 1 arasındaki değerlere atanmasıdır.\n\n> from sklearn.preprocessing import MinMaxScaler\n> mms = MinMaxScaler()\n> X_train_normed = mms.fit_transform(X_train) \n\n*Standartlaştırma:*\n\nDeğişken (özellik) sütunlarının ortalama değeri 0 ve standart sapması 1 olacak şekilde standart normal dağılım oluşturmaktır.\n\n> from sklearn.preprocessing import StandardScaler\n> std = StandardScaler()\n> X_train_std = std.fit_transform(X_train)\n\n\n\n**B) Veri Dönüştürme**\n\nLabel Encode ve OneHot Encoder, kategorik değişkenlerin, tahminlerde kullanılabilmesi için makine öğrenmesi algoritmalarına uygun\nbir forma dönüştürür.\n\n**Label Encoder:**\n Veriyi birebir sayısallaştırır. Her kategorik veriyi sayısal bir değere atar. Verimsiz olduğu için pek kullanılmaz, yerine one OneHotEncoder tercih\n edilir.\n \n**OneHotEncoder**\n\nKategorik veriler için binarizasyon işlemi yapar. Mevcut duruma \"1\", diğer bir duruma ise \"0\" değerini atar.\n\n> from sklearn.preprocessing import OneHotEncoder \n\n> enc_df = pd.DataFrame(df.fit_transform(bridge_df[['columns_name']]).toarray())\n\n> df = df.join(enc_df)\n\n**GetDummies**\n\n Daha esnek bir kullanımı vardır.Tüm kategorik değişkenleri tek seferde sayısal değişkenlere dönüşütürür.\n \n> df = pd.get_dummies(data)\n\n\n\n\n![](https://i.ibb.co/dGxVhns/onehotencoding.jpg)\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## 4. Veri İndirgeme\n\nŞu başlıklar altında toplanırlar:\n\n* Öznitelik/Boyut sayısının azaltılması (dimensionality reduction)\n* Nesne/ Gözlem sayısının azaltılması (numerosity reduction)\n* Çeşitli sıkıştırma teknikleri ile veri hacminin azaltılması (data reduction)\n"},{"metadata":{},"cell_type":"markdown","source":"# Makine Öğrenmesi Algoritmaları\n\nMakine öğrenmesi algoritmalarını supervised(gözetimli) ve unsupervised(gözetimsiz) olarak ikiye ayırırız.\n "},{"metadata":{},"cell_type":"markdown","source":"\n# Gözetimli Öğrenme\n\nGözetimli(supervised) öğrenme ile etiketli(labeled) veriyi eğitiriz. Verisetindeki özellikler için belirlenmiş çıkışlar vardır.\n\n# A) Regresyon Algoritmaları\n\n## Lineer Regresyon(Linear Regression)\n\nVeri boyutundan bağımsız olarak doğrusal ilişki üzerine kurulur. Fakat doğrusal bir ilişki olduğunu kabul etmek çoğu zaman iyi sonuçlar vermeyecektir.\n\n> Y = label, X = label hariç diğer öznitelikler:\n\n> from sklearn.linear_model import LinearRegression\n\n> lin_reg = LinearRegression()\n\n> lin_reg.fit(X,Y)\n\n## Polinomsal Regresyon(Polynomial Regression)\n\nDoğrusal olamayan problemlerde kullanılır ve yüksek başarı için polinom derecesi önemlidir.\n\n> from sklearn.preprocessing import PolynomialFeatures\n\n> poly_reg = PolynomialFeatures(degree = 2)\n\n> lin_reg2.fit(X,Y)\n\n## Support Vector Regression(SVR)\n\nDoğrusal olmayan problemlerde çalışır. Ölçekleme ve doğru kernel fonksiyonu seçimi yapmak önemlidir.\n\n> from sklearn.svm import SVR\n\n> svr_reg = SVR(kernel='rbf')\n\n> svr_reg.fit(X,Y)\n\n## Decision Tree Regression\n\nDoğrusal veya doğrusal olmayan problemlerde de çalışır. KÜçük veri setleri üzerinde\noverfitting olabilir. Karar ağaçlarının öğreniminde iki ana hedef söz konusudur. Sıınıflandırma ağaçları olarak isimlendirilen birinc grupta, veri dizisinin olabildiğince homojen olarak sınıflandırılması; regresyon ağaçları olarak isimlendirilen ikinci grupta ise, tahmin modellerinin kurulması hedeflenmektedir.\n\n* Anlaşılması ve yorumlanmaı son derece kolaydır.\n* Diğer yöntemler verinin normal dağılmasına veya eksik değerlere karşı son derece hassasken, karar ağacı öğrenimi modellerinde kullanılan veri fazla ön işleme gerek duymadan kullanılabilir.\n* Hem sürekli hem de kategorik değerleri bir arada veya ayrı ayrı kullanabilen çeşitli karar ağacı öğrenimi algoritmaları geliştirilmiştir.\n* Çeşitli istatistik testeler kullanılarak, bir modelin güvenilirliği sınanabilir.\n* Ölçeklenebilir özelliği ile veri dizilerinde de kabul edilebilir süreler içerisinde sonuçların elde edilmesini sağlar.\n* Ezbere öğrenme yoğun olduğu için budama yöntemlerinin etkin kullanımı gerekir.\n\n\n> from sklearn.tree import DecisionTreeRegressor\n\n> r_dt = DecisionTreeRegressor(random_state=0)\n\n> r_dt.fit(X,Y)\n\n## Random Forest\n\nÖlçeklemeye ihtiyaç duymaz. Doğrusal veya doğrusal olmayan problemlerde de çalışır. Overfitting riski düşüktür. Çıktıların\nyorumu ve görselleştirmesi nispeten zordur.\n\n> from sklearn.ensemble import RandomForestRegressor\n> rf_reg=RandomForestRegressor(n_estimators = 10,random_state=0)\n> rf_reg.fit(X,Y)\n\n\n# B) Sınıflandırma Algoritmaları\n\n### **Train Test Split**\n\nMakine öğrenmesinin başarısını test etmek amacıyla veri setini eğitim ve test olarak iki parçaya böleriz.\n\n> from sklearn.model_selection import train_test_split\n\n> X_train, X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33, random_state=0)\n\n## Logistic Regression\n\nLineer regresyon analizinin önemli yapı taşlarından biri bağımlı ve bağımsız değişkenlerin sürekli değerler almasıdır. Ancak günlük birçok uygulamada bağımlı değişkenin sadece iki mümkün değer alması beklenir. Bu tip durumlarda kullanılacak yöntemlerden birisi ikil lojistik regresyondur. Bernoulli Dağılımı olarak isimlendirilen kesikli dağılım, bir olayın var/yok, ölü/sağ gibi sadece iki mümkün değer alabilmesini inceler. Lojistik fonksiyonun en önemli özelliği  -∞/+∞ aralığında her türlü değer girdi olarak kabul etmesi ve çıktısının olasılık olarak yorumlanabilecek 0 ile 1 arasında değer üretmesidir.\n\n\n\n> from sklearn.linear_model import LogisticRegression\n\n> logr = LogisticRegression(random_state=0)\n\n> logr.fit(X_train,y_train)\n\n## K-NN\n\nBu algoritmada sınıflandırılmak istenen nesne, öznitelik değerlerinde göre kendisine en yakın komşu veya komşuların sınıfına atanır. İlk olarak yeni tanımlanan X türünün diğer örneklere olan Euclid uzaklıkları hesaplanır, hesaplanan Euclid uzaklıkları küçükten büyüğe doğru sıralanır.\nKomşular uzaklaştıkça sınıflandırmaya etkilerinin de azaltılması amacı ile uygulanan bir yaklaşım, komşuların etkilerinin uzaklıklarına göre ağırlandırılmasıdır. Bu işlem için d uzaklık w ağırlık değeri olmak üzere w = 1/ eşitliği kullanılabilir.\n\n> from sklearn.neighbors import KNeighborsClassifier\n\n> knn = KNeighborsClassifier(n_neighbors=1, metric='minkowski')\n\n> knn.fit(X_train,y_train)\n\n## Kernel SVM\n\nDoğrusal olmayan problemlerde yüksek perfonmaslıdır. Overfitting durumuna karşı duyarlı değildir. Outlier değerlere karşı hassas değildir.\n\n> from sklearn.svm import SVC\n\n> svc = SVC(kernel='rbf')\n\n> svc.fit(X_train,y_train)\n\n## Naive Bayes\n\nVerimli, outlier değerlere karşı hassas değil, doğrusal olmayan problemler üzerinde çlışır ve olasılıksal bir yaklaşımdır. Özelliklerin aynı istatiksel\nanlamlılığa sahip olduğu varsayımını kabul eder.\n\n> from sklearn.naive_bayes import GaussianNB\n\n> gnb = GaussianNB()\n\n> gnb.fit(X_train, y_train)\n\n## Decision Tree Classification\n\n> from sklearn.tree import DecisionTreeClassifier\n\n> dtc = DecisionTreeClassifier(criterion = 'entropy')\n\n>dtc.fit(X_train,y_train)\n\n\n### **Model Predict**\n\nYukarıda kullanılan modeller için X_test veri seti üzerinde predict yaparak sonuçları değerlendirebiliriz.\n\n> y_pred = dtc.predict(X_test)\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Gözetimsiz Öğrenme\n\nVeri setin de bir etiket(label) bulunmaz.\n\n\n# Bölütleme/Kümeleme(Clustering) Yöntemleri\n\nKüme analizi(cluster analysis) veya kümeleme(clustering) en basit tanımı ile veri dizisinde yer alan nesnelerin aynı gruplarda yer alacak şekilde ayrıştırılmasıdır. Küme analizinde, aynı küme içerisinde yer alan nesnelerin olabildiğince birbirleri le bağdaşık(homojen), farklı kümelerde yer alan nesnelerle ise olabildiğince ayrışık(heterojen) olması hedeflenir. Özellikle makine öğrenimi kapsamında geliştirilen kümeleme algoritmaları, örüntü tanıma(pattern recognition), konuşma tanıma(speech recognition), görüntü işlemi(image processing), görüntü işlemede vektör nicemleme(vector quantization) olarak da bilinen veri sıkıştırmada kullanılan spesifik uygulamalarda, tıbbi görüntüleme, suç(crime) ve sosyal ağ(social networks) analizlerinde rol oynamaktadır.\n\n## K-MEANS\n\nİlk olarak kullanıcı tarafından küme sayısı k değerinin belirlenmesi gerekmektedir. K değerinin 2 olarak belirlenmesi durumunda keyfi olarak veya algoritmada yer alan kurallara göre başlangıç küme merkezleri(cluster centroid) seçilir. K-means algoritmasında tüm nesnelerin C1 ve C2 küme merkezlerine olan uzaklıkları kareli Euclid uzaklık ölçüsü kullanılarak hesaplanır. Hesaplanan bu uzaklık değerine göre hangi nesnenin hangi kümede yer alacağı kolayca belirlenebilir. Bu süreç tanımlanan maksimum iterasyon sayısına varıncaya kadar veya merkez değişikleri tanımlanan yakınsama kriterinden daha küçük oluncuya kadar sürdürülür.\n\n> from sklearn.cluster import KMeans\n\n> kmeans = KMeans ( n_clusters = 3, init = 'k-means++')\n\n> kmeans.fit(X)\n\n## Hiyerarşik Kümeleme\n\nHiyerarşik küme analizinin ana çalışma prensibi, benzer öznitelik değerlerine sahip olan nesnelerin adım adım bir araya getirilmesi veya tam tersine bütünden adım adım ayrılmasıdır.\n\n> from sklearn.cluster import AgglomerativeClustering\n> ac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n> Y_tahmin = ac.fit_predict(X)\n"},{"metadata":{},"cell_type":"markdown","source":"# Model Başarılarının Değerlendirilmesi\n\n\n# A) Regresyon Modellerinin Değerlendirilmesi\n\n\n## 1.  R²\n\nEğitilen modelin y veri setini ne kadar açıkladığı sonucunu verir. En sade haliyle doğrusal regresyon modelleri için\nbir uygunluk ölçümüdür. Overfitting durumuna dikkat edildiği takdirde ne kadar yüksekse o kadar iyidir.\n\n> from sklearn.metrics import r2_score\n\n> print(r2_score(y_test, y_pred))\n\n## 2. Mean Absolute Error(MAE)\n\nTahmin edilen değerler ile gerçek değerler arasındaki farktır. Düşük olması modelin daha iyi çalıştığını gösterir.\n\n> from sklearn.metrics import mean_absolute_error\n\n> mean_absolute_error(y_test, y_pred )\n\n## 3. Mean Squared Error(MSE)\n\nTüm veri kümesinde örnek başına ortalama kare kaybıdır. Düşük olması modelin daha iyi çalıştığını gösterir.\n\n> from sklearn.metrics import mean_squared_error\n\n> mean_squared_error(y_true, y_pred)\n\n# B) Sınıflandırma Modellerinin Değerlendirilmesi\n\n\n## 1. Confusion Matrix(Karışıklık Matrisi)\n\nTahmin edilen ve gerçek değerlerin 4 farklı kombinasyonunu içerir.\n\n\n> from sklearn.metrics import confusion_matrix\n> confusion_matrix(y_test, y_pred)\n\n* TP (True positive — Doğru Pozitif): Hastaya hasta demek.\n* FP (False positive — Yanlış Pozitif): Hasta olmayana hasta demek.\n* TN (True negative — Doğru Negatif): Hasta olmayana hasta değil demek.\n* FN (False negative — Yanlış Negatif): Hasta olana hasta değil demek.\n\n\n![cm](https://i.ibb.co/3rHT5Tn/cmm.png)\n\n* Doğruluk(Accuracy) = Doğru olarak sınıflandırılan örneklerin yüzdesidir.\n\n> from sklearn.metrics import accuracy_score\n\n> accuracy_score(y_test, y_pred)\n\n* Duyarlılık(Recall) = \n\n> from sklearn.metrics import recall_score\n> recall_score(y_test, y_pred)\n\n![recall](https://i.ibb.co/HVHZb0m/recall.png)\n\n* Kesinlik(Precision) =\n\n> from sklearn.metrics import recall_score\n\n>recall_score(y_test, y_pred)\n\n![precision](https://i.ibb.co/Px857vg/precison.png)\n\n* F1 Score = Modelimizin kesinliğinin bir ölçüsüdür. 1(en iyi durum) ile 0 arasında değer alabilir.\n\n![f1](https://i.ibb.co/T1QD8Hg/f1.png)\n\n> from sklearn.metrics import f1_score\n> f1_score(y_true, y_pred)\n\n**Değerlendirme**\n \n Aşağıdaki confusion matrix grafiğinde 13, 10 ve 9 değerleri doğru tahminlerimizin sayılarını vermektedir.\n\n![cm2](https://i.ibb.co/XCs7wB6/cm.png)\n\n\n\n\n## 2. AUC-ROC Eğrisi\n\nROC bir olasılık eğrisidir ve AUC ayrılabilirliğin derecesini veya ölçüsünü temsil eder. AUC, ROC eğrisinin altında kalan alandır. Modelleri sınıflar arasında ne kadar ayırt edebildiğini anlatır.\n\n![auc-roc](https://i.ibb.co/4mC6S0c/ROC-image.png)\n"},{"metadata":{},"cell_type":"markdown","source":"# Model Seçimi\n\n\n## Ensemble Learning\n\n\n Makine öğreniminde ensemble öğrenim(ensemble learning), elde edilecek sonuçların performansının arttırılması amacı ile farklı öğrenme algoritmalarının bir arada kullanılmasıdır. Sonra bu modeller oylanır.Ensemble öğrenimde de bagging, boosting, stacking, error-correcting, output codes gibi çok farklı yaklaşımlar geliştirilmiştir.Bagging ve bootstrap aggregation, karar ağaçları öğreniminde ilk dönem ensemble yöntemleridir. Öğrenim verisinin varyansının azaltılması ve ezbere öğrenme sorununu gidermek amacı ile çok sayıda örnekleme yapılarak, bunlardan üretilen karar ağaçlarından birinin oyalama sonucunda seçilmesi sürecidir.Ensemble öğrenimi hızlandırabilmek için paralel hesaplama(parallel computing) kapsamında paralel CPU kullanılabilir.\nBoosting ise bagging yaklaşımının bir adım daha geliştirilmiş halidir. Burada her bir modele verilen öneme göre ağırlık değerleri atanır. Boosting yaklaşımını kullanan yöntemlerden biri AdaBoost algoritmasıdır.\n\n\n## K-Fold Cross Validation(K-Katlamalı Çapraz Doğrulama)\n\n Sınıflandırma modellerinin değerlendirilmesi ve modelin eğitilmesi için veri setini parçalara ayırma yöntemlerinden biridir. Veri seti eğitim-test olarak ikiye ayrılır. Eğitim seti k adet alt parçaya ayılır. Bir parça dışarıda bırakılır ve diğer parçaların doğruluğu dışarıda bırakılan parça ile test edilir. Her bir iterasyonda başka bir parça dışarıda bırakılır. Bu hataların ortalaması alındığında ortaya validasyon hatası çıkar. Büyük hacimli veri setlerinde hesaplama ve zaman yönünden maliyetli olabilir.\n \n> from sklearn.model_selection import cross_val_score\n\n> basari = cross_val_score(X_train, y_train, cv = 5)\n\n> basari.mean()\n\n> basari.std()\n\nCross Validation ile modelimizin sağlamlığını test etmiş oluruz. Overfitting gibi istenmeyen bir durumla karşı karşıya olup olmadığımızı cross validation işlemi sonucunda elde edilen sonuçların accuracy gibi metriclerine bakarak anlayabiliriz.\n\n\n\n![k-fold](https://i.ibb.co/xfRVP8N/kfold.pngg)\n\n\n## Grid Search\n\nKullanmak istediğimiz modelin en iyi parametrelerini bulur. Yani modeli optimize etmeye yarar.\n\n> from sklearn.model_selection import GridSearchCV\n\n\n> gs = GridSearchCV(classffier = classifier(eğittiğimiz model)\n                    param_grid = p,\n                  scoring =  'accuracy',\n                  cv = 10,(cross-validation = kaç katlamalı)\n                  n_jobs = -1)\n\n\n> grid_search = gs.fit(X_train,y_train)\n\n> eniyisonuc = grid_search.best_score_\n\n> eniyiparametreler = grid_search.best_params_\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# BIAS - VARIANCE (YANLILIK VE VARYANS)\n\nModelimizi eğittikten sonra eğitim ve test hatası arasındaki duruma göre modelimizde bias veya variance durumunu anlayabiliriz. Yüksek yanlılık(high bias) overfitting olduğunu gösterir. Yüksek varyans(high variance) ise underfitting(öğrenememe) olduğunu gösterir.\n\n![bias](https://i.ibb.co/ftCjBRW/bias.png)\n\n\n* Eğitim hatası %1 ve test hatası %11 olduğunu varsayarsak, yüksek varyans olduğu anlaşılır. Bu durumda modelimiz ezberleme(overfitting) yapmıştır.\n\n* Eğitim hatası %15 ve test hatası %30 ise eğitim ve test sonuçlarıda iyi değil demektir. Böyle bir durumda yüksek yanlılık ve yüksek varyans vardır.\n\n* Eğitim hatası %15 ve test hatası %16 olduğu durumda, eğitim setinin bile verileri sınıflandırmada yetersiz kaldığı görünür. Bu durum da yüksek yanlılık vardır.\n\n* Eğitim hatası %0.5 ve test hatası %1 ise düşük yanlılık ve düşük varyans var demektir. Bu istenilen bir sonuçtur.\n\n\n#### **Bias ve Variance sorunlarını çözmek için tavsiyeler:**\n\n1. High Bias:\n\n Derin öğrenmede bu durumla karşılaşıyorsak daha büyük bir ağ sorunumuzu çözecektir. Klasik makine öğrenmesi algoritmalarında ise model parametrelerini optimize etmek gerekir.\n \n2. High Variance:\n\n Daha fazla veri kullanarak bu sorunu çözebiliriz."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}